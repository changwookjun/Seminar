{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 설립 배경\n",
    "1. 엘론 머스크와 페이팔 공동창업자인 피터 틸, 링크드인 CEO인 리드 호프만 등이 OpenAI.com을 설립\n",
    "2. OpenAI 는 비영리 단체로 AI 관한 모든 연구 결과물을 공개할 것이라고 하고 이를 위해 10억달러를 투자\n",
    "3. 엘론 머스크는 구글, 페이스북, 마이크로소프트, 애플 같은 IT 공룡들이 주도하는 인공지능 연구에 대해 많이 우려 하며\n",
    "   몇몇 회사나 국가에서 인공지능을 독점하는 병폐를 막기위해 OpenAI 를 설립\n",
    "4. OpenAI 가 초기 집중할 연구영역은 인공지능 분야로 딥러닝(Deep Learning)에 관한 연구\n",
    "5. 딥러닝을 위한 하드웨어 인프라를 아마존에서 제공\n",
    "6. OpenAI 라는 공공성을 내세우고 구글, 페이스북 등으로부터 좋은 연구인력을 모음\n",
    "   (몬트리올 대학교의 Yoshua Bengio 교수,Geoff Hinton은 구글, Yann LeCun은 페이스북 업계) 최고의 연구자 목록을 뽑아 컨택하기 시작 \n",
    "   10명중 9명이 OpenAI에 합류, OpenAI는 이보다 하루 더 전에 UC 버클리의 Pieter Abbeel 교수를 풀타임으로 고용 Pieter Abbeel 교수는 강화학습    개발을 위한 프레임워크인 rllab을 만들었으며 로봇과 강화학습 분야에 권위자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 연구 목적(4가지)\n",
    "\n",
    "<ol>\n",
    "<li>연구 성과 측정</li>\n",
    "<li>가정용 로봇</li>\n",
    "<li>NLP 기반 에이전트</li>\n",
    "<li>게임 에이전트</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI First Work - Gym \n",
    "\n",
    "- <a href=\"https://openai.com/\" target=\"_blank\">OpenAI</a>가 첫번째 작업물인 <a href=\"https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5\" target=\"_blank\">강화학습</a>(Reinforcement Learning) 툴킷인 <a href=\"https://gym.openai.com\" target=\"_blank\">Gym</a>을 <a href=\"https://openai.com/blog/openai-gym-beta/\" target=\"_blank\">블로그</a>를 통해 공개\n",
    "- 소스는 <a href=\"https://github.com/openai/gym\" target=\"_blank\">깃허브</a>를 통해 확인 가능\n",
    "- Gym은 강화학습 알고리즘을 평가하고 개발하는 데 도움을 주는 툴킷\n",
    "- Gym은 파이썬으로 작성되어 있고 특별한 프레임워크에 종속적이지 않으므로 <a href=\"https://tensorflowkorea.wordpress.com/2016/04/28/first-contact-with-tensorflow/\">TensorFlow</a>나 <a href=\"https://tensorflowkorea.wordpress.com/2016/03/28/theano-0-8-%eb%a6%b4%eb%a6%ac%ec%a6%88/\">Theano</a> 사용 가능 \n",
    "- 다만 현재는 파이썬 2.7.x 버전용\n",
    "- Gym은 강화학습을 위한 다양한 테스트 환경(environment)을 제공하여 누구나 자신의 에이전트(agent)를 개발할 수 있도록 하고 그 결과를 업로드하여 다 같이 <a href=\"https://gym.openai.com/envs\" target=\"_blank\">공유</a>할 수 있도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Setting 방법\n",
    " ### Setting 환경 \n",
    "   - AWS Hardware 용량 : 8G / 메모리1G  \n",
    "   - OS : Ubuntu 14.04.3 LTS \n",
    "   - Python 2.7\n",
    " ### gym Installation\n",
    "   - git clone https://github.com/openai/gym.git\n",
    "   - cd gym\n",
    "   - pip install -e . \n",
    "   - apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
    " ### Rendering on a server \n",
    "   - xvfb-run -s \"-screen 0 1400x900x24\" bash\n",
    "   \n",
    "    <p>port gym <br>\n",
    "    env = gym.make('CartPole-v0') <br>\n",
    "    env.reset() <br>\n",
    "    env.render() </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CartPole예제 이해하기\n",
    "\n",
    "<p><span style=\"font-size:16px;\">OpenAI 공식 사이트는 다음과 같습니다.&nbsp;</span></p>\n",
    "\n",
    "<p><span style=\"font-size:16px;\"><a href=\"https://gym.openai.com/envs/CartPole-v0\">https://gym.openai.com/envs/CartPole-v0</a><br>\n",
    "여러가지 예제들 중에서 사이트에서도 OpenAI를 소개하는 예제로서 Cartpole예제를 사용하였습니다.</span></p>\n",
    "\n",
    "<p><span style=\"font-size:16px;\">가장 간단한 예제인 것 같아서 Cartpole로 강화 학습 문제에 대해서 이해해보려고 합니다.</span></p>\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><p><strong><span style=\"font-size:22px;\">1. What is Reinforcement Learning</span></strong></p>\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">위키피디아에서 강화 학습을 다음과 같이 설명하고 있습니다.&nbsp;</span></p>\n",
    "\n",
    "<blockquote>\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\"><b>강화 학습</b>(Reinforcement learning)은&nbsp;이 다루는 문제 중에서 다음과 같이 기술 </span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">되는 것을 다룬다. 어떤&nbsp;<b>환경</b>을 탐색하는&nbsp;<b>에이전트</b>가 현재의&nbsp;<b>상태</b>를 인식하여 </span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">어떤&nbsp;<b>행동</b>을 취한다. 그러면 그 에이전트는 환경으로부터&nbsp;<b>포상</b>을 얻게 된다. 포</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">상은 양수와 음수 둘 다 가능하다. 강화 학습의 알고리즘은 그 에이전트가 앞으</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">로 누적될 포상을 최대화 하는 일련의 행동으로 정의되는&nbsp;<b>정책</b>을 찾는 방법이다.</span></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "위에 나온 Reinforcement Learning 을 쉽게 제 생각을 정리 해봅니다. <br>\n",
    "만약 인간이 태어나서 이동하는 과정에 비교한다면  <br>\n",
    "목표 : 이동  <br>\n",
    "1. 최초 이동 불가  <br>\n",
    "2. 기어 간다  <br>\n",
    "3. 걸어 간다  <br>\n",
    "4. 뛰어 간다. <br>\n",
    "1 ~ 4 행위들을 하기 위해서는 실패와 성공이라는 반복적 학습을 통해서 인간이 할수 있는 이동 중 가장 빠른 \"뛰어간다.\" 까지 도달하는 부분 <br>\n",
    "그리고 외적 보상 (예: 부모님의 칭찬, 물질적 보상 등) 과 내적 보상(예: 자기 만족 , 성취감 등)을 통한 빠른 목표 수렴등이 <br>\n",
    "Reinforcement Learning과 인간의 학습 방법은 비슷한거 같습니다. <br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 위키피디아의 강화학습 정의를 우리가 배운 것으로 표현하자면, 특정 Environment에 Agent가 놓여있는데 Agent는 그 Environment를 MDP로 이해합니다. MDP란 Markov Decision Process의 약자로서 state, action, station probability matrix, reward, discounted factor로 이루어져있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img class=\"alignnone size-full wp-image-3722\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Markov_Decision_Process_example.png/400px-Markov_Decision_Process_example.png\" alt=\"Markov_Decision_Process\"></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><img src=\"http://www.modulabs.co.kr/files/attach/images/334/136/002/9864ef6a012bcbff9249a3805b06035d.png\" alt=\"1.png\" style=\"height:319px;width:600px;\" rel=\"xe_gallery\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###State\n",
    "state는 agent가 인식하는 자신의 상태입니다.<br>\n",
    "OpenAI에도 있는 atari game같은 경우에는 게임화면 자체, 즉 pixel이 agent가 인식하는 state가 됩니다. <br>\n",
    "\n",
    "- ###Action\n",
    "agent의 역할은 무엇일까요? environment에서 특정 state에 갔을 때 action을 지시하는 것입니다. <br>\n",
    "\n",
    "- ###Reward\n",
    "이렇게 action을 취하면 그에 따른 reward를 \"environment\"가 agent에게 보여줍니다. <br>\n",
    "정확히 말하면 agent가 observe하는 거죠. <br> \n",
    "\n",
    "- ###State transition probability matrix\n",
    "<br>\n",
    "\n",
    " 이렇듯 agent는 environment와 상호작용을 하는데 그 그림은 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-left:40px;\"><img src=\"https://gym.openai.com/static/img/tutorial/aeloop.svg\" alt=\"2.png\" style=\"\" rel=\"xe_gallery\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent가 observation을 통해서 자신의 state를 알게되면 그 state에 맞는 action을 취하게 됩니다. <br>\n",
    "학습을 하지 않은 초기에는 보통 random action을 취합니다. <br>\n",
    "그러면 environment가 agent에게 reward와 다음 state를 알려주게 됩니다. <br> \n",
    "\n",
    "- ###Policy\n",
    "뜻 그대로 풀이하자면 \"정책\"입니다. <br>\n",
    "위에서 말했듯이 agent는 어떤 state에 도착하면 action을 결정하는 데 어떤 state에서 어떤 action을 할 지를 policy라고 합니다. <br> \n",
    "결국에 강화학습의 목적은 optimal policy ( accumulative reward = return 을 최대화하는 policy)를 찾는 것입니다.  <br>\n",
    "\n",
    "- ###State-value function\n",
    "agent가 state 1에 있다고 가정해봅시다. 거기서부터 쭉 action을 취해가면서 이동 그에 따라서 reward를 받는 것들을 쭉 기억한다고 해봅시다. <br>\n",
    "끝이 있는 episode라고 가정했을 때 episode가 끝났을 때 state 1에서부터 받았던 reward를 다 더할 수 있을 겁니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-left:40px;\"><img src=\"http://www.modulabs.co.kr/files/attach/images/334/136/002/2f32323a0ff14183c045cfb04744ab73.png\" alt=\"4.png\" style=\"\" rel=\"xe_gallery\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 때 단순히 순간 순간 action을 취하면서 받았던 reward를 더하는 것이 아니고 discount factor를 곱해줘서 더해줍니다. <br>\n",
    "만약 discount factor가 없이 그냥 더할 경우 0.1씩 계속 더해도 무한대이고 1씩 계속 더해도 무한대로 return이 가기때문에 사실상 비교할 수가 없습니다. <br> \n",
    "Return에 대한 식은 위와 같고 이 return의 expectation이 state-value function입니다. <br>\n",
    "즉, 지금 상태의 가치죠. <br> \n",
    "따라서 agent는 다음 state들의 가치를 보고서 높은 가치가 있는 state로 가는 것을 선호하게 되는 것입니다. <br>\n",
    "\n",
    "- ###Action-value function\n",
    "하지만 막상 그렇게 하려고 하니 다음 state들에 대한 정보를 다 알아야하고 그 state로 가려면 어떻게 해야하는 지<br>\n",
    "(예를 들면 화살을 쏠 때 바람이 부니까 조금 오른쪽으로 쏜다라던지)도 알아야합니다.<br> \n",
    "이제 와서야 말하는데 이 environment의 MDP라는 모델을 몰라도 학습한다는 것이 강화학습의 핵심개념입니다.<br> \n",
    "그것보다는 왼쪽으로 발을 디디는 것이 나은 건지 오른쪽으로 발을 디디는 것이 나은 건지에 대한 정보만 알고 있으면 행동하기에 편할 것입니다.<br> \n",
    "여기서 Action-value function의 개념이 나오게 됩니다.<br>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;line-height:24px;\">&nbsp; </span><img src=\"http://www.modulabs.co.kr/files/attach/images/334/136/002/e7b067d294a64c295cd120d1cdf33e20.png\" alt=\"5.png\" style=\"\" rel=\"xe_gallery\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 어떤 state s에서 action a를 취할 경우의 받을 return에 대한 기대값으로서 어떤 행동을 했을 때 얼마나 좋을 것인가에 대한 값이죠!<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole예제\n",
    "cartpole 예제의 목표는 최대한 오랫동안 막대기를 세우고 있는 것입니다. \n",
    "이 문제에 대한 정의는 OpenAI사이트에서 소개되고 있습니다\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\"><a href=\"https://gym.openai.com/envs/CartPole-v0\" target=\"_blank\">https://gym.openai.com/envs/CartPole-v0</a></span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><img src=\"http://www.modulabs.co.kr/files/attach/images/334/136/002/6cced5c096682c8a561f6a7825e809e7.png\" alt=\"3.png\" style=\"\" rel=\"xe_gallery\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">이 시스템은 track과 cart와 joint, pole로 이루어져 있습니다. cart는 track위에서 마찰없이 움직이며 cart와 pole은 자유로운 joint로 연결되어있어서 pole이 joint를 중심으로 자유롭게 회전할 수 있습니다. 이 시스템에는 중력이 작용하고 있어서 가만히 놔두면 pole은 밑으로 떨어지게 됩니다. 따라서 agent는 cart는 왼쪽 아니면 오른쪽으로 움직여서 pole을 세워놓아야합니다. reward는 pole이 세워져있는 시간인데 애초에 이 episode는&nbsp;pole이 수직에서 15도 떨어지거나 가운데로부터 2.4units 만큼 떨어진다면 끝나게 되어있습니다. 따라서 episode가 유지되는 시간자체가 reward가 됩니다.&nbsp;</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">여기서는 discount factor가 없습니다.&nbsp;</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">이 문제는 1983년도의 <em>\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Problems-Andrew G.&nbsp;Barto, Richard S. Sutton , Charles W. Anderson\"&nbsp;</em>에 정의되어 있습니다. 이 문제에서 말하는 state는 다음과 같습니다.</span></p>\n",
    "\n",
    "<ul><li style=\"margin-left:40px;\"><span style=\"font-size:16px;\">x : position of cart on the track</span></li>\n",
    "\t<li style=\"margin-left:40px;\"><span style=\"font-size:16px;\">θ&nbsp;: angle of the pole with the vertical</span></li>\n",
    "\t<li style=\"margin-left:40px;\"><span style=\"font-size:16px;\">dx/dt : cart velocity</span></li>\n",
    "\t<li style=\"margin-left:40px;\"><span style=\"font-size:16px;\">dθ/dt : rate of change of the angle</span></li>\n",
    "</ul><p style=\"margin-left:40px;\"><span style=\"font-size:16px;line-height:24px;\">즉, agent는 state로서 이 네가지를 observation하게 됩니다. 이 것을 실재로 코드를 실행시켜서 확인해보면 다음과 같습니다.&nbsp;</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><a href=\"https://gym.openai.com/docs\" target=\"_blank\">https://gym.openai.com/docs</a></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><img src=\"http://www.modulabs.co.kr/files/attach/images/334/136/002/37c86507a81f9efa64c5778c05834e56.png\" alt=\"7.png\" style=\"\" rel=\"xe_gallery\"></p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">이 코드에서 보면 밑의 프린트되는 행렬은 observation으로서 각 time-step마다의 state를 보여줍니다. 첫번째 줄&nbsp;</span><span style=\"font-size:16px;\">[-0.061586 -0.75893141 0.05793238 1.15547541]에서 각각</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;line-height:24px;\">x : -0.061586</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">θ : -0.75893141</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">dx/dt : 0.05793238</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">dθ/dt : 1.15547541</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;\">와 같이 됩니다.</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;line-height:24px;\">&nbsp;agent는 이러한&nbsp;특정한 state에서 action을 결정하게 되는데 action은 cart에 impulse force를 왼쪽 아니면 오른쪽으로 가하는 것으로&nbsp;0과 1로서 각각 정의됩니다.&nbsp;하지만 결국에 action을 계속 취하려면 모든 state에 대한 q-value값들을 알고 있어야 하는데&nbsp;그러면 사실 말도 안되는 숫자의 state에 대한 정보를 기억하고 있어야하고 학습이 상당히 느려지게 됩니다. 따라서 여기에 approximation이 들어가게 됩니다.</span></p>\n",
    "\n",
    "<p style=\"margin-left:40px;\">&nbsp;</p>\n",
    "\n",
    "<p style=\"margin-left:40px;\"><span style=\"font-size:16px;line-height:24px;\">&nbsp;</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN-tensorflow\n",
    "https://gist.github.com/carpedm20/2035298fdc3313e16aae0c6bb690bada#new_comment_field\n",
    "\n",
    "OOM 발생 ㅠㅠ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How to add new environments to Gym\n",
    "1. \"/gym/envs\" 해당 folder에 새로운 environment를 추가\n",
    "2. 만약 my_collection 폴더에 my_awesome_env.py 를 만든다면  \"/gym/envs/my_collection/__init__.py\" init 파일을 만들고 \n",
    "   \"from gym.envs.my_collection.my_awesome_env import MyEnv\"를 한다.\n",
    "3. \"/gym/envs/__init__.py\" 에 등록 한다.\n",
    "   <p>register( <br>\n",
    "    id='MyEnv-v0', <br>\n",
    "    entry_point='gym.envs.my_collection:MyEnv', <br>\n",
    "    )</p>\n",
    "4. \"/gym/scoreboard/init.py\"에 environments를 추가 한다.\n",
    "    <p> \n",
    "    add_task(<br>\n",
    "    id='MyEnv-v0',<br>\n",
    "    summary=\"Super cool environment\", <br>\n",
    "    group='my_collection',<br>\n",
    "    contributor='mygithubhandle',<br>\n",
    "    )<br>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recording and uploading results\n",
    "\n",
    "학습한 Data 와 랜더링 데이터를 OpenAI의 서버로 upload 할 수 있다.\n",
    "방법은 아래와 같다.\n",
    "\n",
    "Source 부분\n",
    "import gym\n",
    "gym.upload('/tmp/cartpole-experiment-1', api_key='sk_ReQPqzlQXawG15DjyZang')\n",
    "\n",
    "api_key를 받는 방법\n",
    "1. https://gym.openai.com/ 에 로그인 (로그인은 GitHub 계정으로 )\n",
    "2. 로그인이 되면 프로필을 누르면 \"Your API Key is\" 부분에 발급 됨\n",
    "\n",
    "Results는 아래와 같다.\n",
    "\n",
    "[2016-04-22 23:16:03,123] Uploading 20 episodes of training data\n",
    "[2016-04-22 23:16:04,194] Uploading videos of 2 training episodes (6306 bytes)\n",
    "[2016-04-22 23:16:04,437] Creating evaluation object on the server with learning curve and training video\n",
    "[2016-04-22 23:16:04,677]\n",
    "****************************************************\n",
    "You successfully uploaded your agent evaluation to\n",
    "OpenAI Gym! You can find it at:\n",
    "\n",
    "    https://gym.openai.com/evaluations/eval_tmX7tssiRVtYzZk0PlWhKA\n",
    "    https://gym.openai.com/evaluations/eval_akcSNfmiSAWisYExWJaCQ#reproducibility\n",
    "****************************************************\n",
    "\n",
    "\n",
    "#Evaluations\n",
    "\n",
    "평가 받고 싶다면  Attach 하면 된다.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "      <p> ... </p>\n",
    "\n",
    "      <p>\n",
    "\t<label for=\"gist\">Gist URL</label> <input disabled id=\"gist\" name=\"gist\" placeholder=\"https://gist.github.com/cwjun/...\" size=\"50\" type=\"text\" value=\"\"> <input type=\"submit\" class=\"btn btn-med btn-primary\" value=\"Attach Gist\" disabled=\"true\">\n",
    "      </p>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참조 Site\n",
    "https://openai.com/blog/openai-gym-beta/\n",
    "\n",
    "https://gym.openai.com/\n",
    "\n",
    "https://tensorflowkorea.wordpress.com/tag/openai/\n",
    "\n",
    "https://github.com/openai/gym/\n",
    "\n",
    "http://www.modulabs.co.kr/index.php?mid=RL_library&document_srl=2136\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
