{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleDS\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dialog System**\n",
    "![General RL](./simpleds/dialogsystem.png)\n",
    "\n",
    "*A Simple Deep Reinforcement Learning Dialogue System*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleDS는 심층적 인 학습을 통해 교육 된 간단한 대화 시스템입니다. \n",
    "응답에서 직접 대화 동작을 선택합니다. \n",
    "**동기는 대화 상대를 가능한 한 인간의 개입없이 훈련시키는 것입니다.**\n",
    "이 시스템은 학습 Agent(JavaScript)가 \"서버\" 역할을 하고 Enviroment(Java)이 \n",
    "\"클라이언트\"역할을하는 **클라이언트 - 서버** 아키텍처에서 실행됩니다.\n",
    "서버는 **메시지 교환을 통해 통신**하며, 여기서 서버는 클라이언트에게 **Action**을 알리고 \n",
    "클라이언트는 사용 Action, Enviroment State 및 observation된 Reword를 서버에 알립니다.\n",
    "server가 Agent이고 client가  enviroment 이다.\n",
    "**SimpleDS는 ConvNetJS http://cs.stanford.edu/people/karpathy/convnetjs/ 를 기반으로하며, experience replay로 깊은 Q-Learning 알고리즘으로 구현**합니다\n",
    "**SimpleDS는 멀티 스레드를 지원하는 ConvNetJS의 상단에있는 대화 시스템입니다.\n",
    "클라이언트 - 서버 프로세싱, Close Domain을 통한 빠른 학습 등이 있습니다.**\n",
    "이 시스템은 Google Speech Recogniser를 사용하여 시뮬레이션 및 실제 대화를 통해 테스트되었습니다. \n",
    "또한 영어, 독일어 및 스페인어의 세 가지 언어로 테스트 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOFTWARE\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ubuntu 14.10.4 / Mac OS X 10.10\n",
    "\n",
    "Java 1.8.0 or higher\n",
    "\n",
    "Ant 1.9.3 or higher\n",
    "[http://ssad.tistory.com/entry/ANT%EB%9E%80](http://ssad.tistory.com/entry/ANT%EB%9E%80)\n",
    "\n",
    "Node 0.10.25 or higher\n",
    "[http://codingdojang.com/scode/265](http://codingdojang.com/scode/265)\n",
    "\n",
    "Octave 3.8.0 or higher (수치해석용 소프트웨어)\n",
    "[http://knot.tistory.com/101](http://knot.tistory.com/101)\n",
    "\n",
    "Android 4.4.3 (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the system directly from the command line:\n",
    "\n",
    "git clone https://github.com/cuayahuitl/SimpleDS.git\n",
    "\n",
    "You can also download the system as a zip file using the following URL, and then unzip it in your path of preference.\n",
    "https://github.com/cuayahuitl/SimpleDS/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXECUTION\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의 사항 nodejs가 아니라 node로 명령을 실행 해야 한다.**\n",
    "\n",
    "cd YourPath/SimpleDS\n",
    "\n",
    "scripts/run.sh train\n",
    "\n",
    "[From the command line, press Ctrl+C for termination]\n",
    "or\n",
    "\n",
    "cd YourPath/SimpleDS\n",
    "\n",
    "scripts/run.sh test\n",
    "\n",
    "[From the command line, press Ctrl+C for termination]\n",
    "Alternatively (recommended), you can run the system from two terminals:\n",
    "\n",
    "Terminal1:YourPath/SimpleDS>ant SimpleDS\n",
    "\n",
    "Terminal2:YourPath/SimpleDS/web/main>node runclient.js (train|test) [num_dialogues] [-v|-nv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualise a learning curve of the SimpleDS agent according to number of learning steps in the x-axis and average reward + learning time in the y-axis. Learning curves can be generated for newly trained or pre-trained policies in the currently supported languages (English, German and Spanish).\n",
    "\n",
    "cd YourPath/SimpleDS\n",
    "\n",
    "octave scripts/plotdata.m results/english/simpleds-output.txt **(Agent 쪽 Data)**\n",
    "\n",
    "[From the command line, press the space bar key for termination]\n",
    "or\n",
    "\n",
    "cd YourPath/SimpleDS\n",
    "\n",
    "octave scripts/plotdata.m results/english/simpleds-output.txt results/english/simpleds-output.png**(Reword / Time 상관 그래프)**\n",
    "\n",
    "[From the command line, press the space bar key for termination]\n",
    "The latter generates an image of the plot in png (Portable Network Graphics) format. The file plotdata.m can also be used from Matlab if that software is prefered. The following learning curves (available from YourPath/results//.png) can be obtained with the default parameters for the supported languages: English, German and Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURATION\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file \"YourPath/SimpleDialogueSystem/config.txt\" has the following parameters:\n",
    "\n",
    "Dialogues=Number of dialogues for training/test (positive integer)\n",
    "\n",
    "Verbose=Shows compressed information or detailed info (false or true)\n",
    "\n",
    "Language=Defines the (spoken) language to use (english, german, spanish)\n",
    "\n",
    "SysResponses=Path and file name of system responses (e.g. resources/SysResponses.txt)\n",
    "\n",
    "UsrResponses=Path and file name of system responses (e.g. resources/UsrResponses.txt)\n",
    "\n",
    "SlotValues=Slot-value pairs of the system (e.g. resources/SlotValues.txt)\n",
    "\n",
    "DemonstrationsPath=Path to the demonstration dialogues (e.g. data/)\n",
    "\n",
    "DemonstrationsFile=Pointer to training instances from demonstrations (models/demonstrations.arff)\n",
    "\n",
    "MinimumProbability=Minimum probability (>=0) for probable actions considered for action-selection\n",
    "\n",
    "SlotsToConfirm=Number of slots to confirm (positive integer, e.g. 3)\n",
    "\n",
    "OutputPath=The directory where the output files (policy and metrics) will be stored\n",
    "\n",
    "NoiseLevel=Scores under this level (<=0.2) would receive distorsion to model noisy recognition\n",
    "\n",
    "AddressPort=Address and port of the client socket (e.g. ws://localhost:8082/simpleds)\n",
    "\n",
    "SavingFrequency=This number defines the frequency for policiy/output saving (positive integer)\n",
    "\n",
    "NumInputs=This number defines the number of input nodes of the neural net (positive integer)\n",
    "\n",
    "NumActions=This number defines the number of actions of the agent (positive integer)\n",
    "\n",
    "LearningSteps=This number defines the number of time steps during learning (positive integer)\n",
    "\n",
    "ExperienceSize=This number defines the size of the experience replay moemory (positive integer)\n",
    "\n",
    "BurningSteps=This number defines the time steps with random action selection (positive integer)\n",
    "\n",
    "DiscountFactor=This number defines gamma parameter also known as discount factor (real number)\n",
    "\n",
    "MinimumEpsilon=This number defines the minimum epsilon during learning (real number)\n",
    "\n",
    "BatchSize=This number defines the batch size (positive integer, e.g. 32 or 64)\n",
    "\n",
    "AndroidSupport=This variable is used to test dialogues with a real speech recogniser (true or false)\n",
    "\n",
    "SocketServerPort=This number defines the socket port used for communication with Android (positive integer)\n",
    "\n",
    "You may want to set Verbose=false during training and Verbose=true during tests. You may also want to set a high number of dialogues during training (e.g. Dialogues=2000) and a low one during tests (e.g. Dialogues=1). You may want to change the system/user responses if you want different verbalisations. If this is the case, then you will also want to update the demonstration dialogues in the folder YourPath/SimpleDS/data/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAPER Playing Atari with Deep Reinforcement Learning\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorism**\n",
    "![General RL](./simpleds/generalRL.png)\n",
    "\n",
    "![Algorism](./simpleds/DQN_Algorism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAPER Deep Reinforcement Learning for Multi-Domain Dialogue Systems\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Networks (DQN)와 같은 표준 심화 학습 학습 방법 여러 작업 (도메인)이 확장 성 문제에 직면합니다. \n",
    "우리는 방법을 제안한다.\n",
    "NDQN이라고하는 다중 도메인 대화 정책 학습을 위해 식당의 영역에서 정보를 구사하는 대화 시스템 호텔에 적용하여\n",
    "DQN 대 NDQN을 비교 한 실험 결과 시뮬레이션을 사용하여 제안 된 방법이 더 나은 확장 성과 다중 도메인 대화 시스템의 동작을 최적화 하였다.\n",
    "\n",
    "divide-and-conquer 접근법을 사용\n",
    "![divice and conquer Algorism](./simpleds/device_conquer_algorithm.png)\n",
    "![ndqn](./simpleds/ndqn.png)\n",
    "\n",
    "그림 1 : 유연한 상호 작용이 가능한 다중 도메인 DRL 에이전트. 도메인을 연결하는 파선 화살표\n",
    "상호 작용에서 엄격한 구조를 피하기 위해 도메인 간의 유연한 전환을 나타냅니다.\n",
    "의사 결정을 위해 모든 정책이 고려되지만, 하나의 도메인 만 실행될 수 있습니다.\n",
    "이전 도메인이 상호 작용을 재개하기 위해 실행을 계속할 수 있음을 암시하는 시간\n",
    "\n",
    "![ndqn](./simpleds/id.png)\n",
    "단어를 '$'을 결합하여 표시 \n",
    "정책을 더 빨리 학습하고, 더 많은 확장 성을 제공\n",
    "큰 어휘가있는 시스템의 경우, 슬롯 값이 변경되면 정책을 재교육할 필요\n",
    "슬롯 값을 슬롯 ID로 대체 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State Spaces**\n",
    "각 학습의 어휘에 따라 단어 기반 기능 Agent - 동의어가 없는 177개의 단어 와 동의어가 있는 150개의 고유 단어 포함 (식당 Domain)\n",
    "\n",
    "**Action Spaces**\n",
    "여기에는 표적 Domain에 대한 대화 행동 (현재 69 개의 고유 한 행동)이 포함\n",
    "예로 매개 변수가 없는 대화 행동은 Salutation (), Request (), AskFor (), Apology (), ExpConfirm (), ImpConfirm (), Retrieve (), Provide () 등이 있다.\n",
    "전체 액션 세트로 학습하는 대신, 우리의 프레임 워크는 제한된 유효한 동작 집합에 대해서만 학습 업데이트를 적용하여 이러한 동작은 파생됩니다.\n",
    "나이브 베이즈 (Naive Bayes) 분류 자로부터의 가장 가능성있는 행동 P r (a | s)> 0.0001**(확장 성 목적). **\n",
    "![PROOF](./simpleds/proof.png)\n",
    "![PROOF EXAM](./simpleds/proof_exam.png)\n",
    "\n",
    "**State Transition Functions**\n",
    "부재중 인 경우 0, 존재하는 경우 1입니다 (히트 또는 미스)\n",
    "사용자 응답 시끄러운 사용자 응답의 신뢰 수준 [0..1]에 해당 합니다.\n",
    "시스템 응답이 생성되는 동안 확률 론적 템플릿으로부터 사용자 응답은 준 임의적 사용자 행동으로부터 생성됩니다.\n",
    "이러한 요소를 통해 에이전트 교육을위한 막대한 양의 대화를 생성 할 수 있습니다.\n",
    "\n",
    "**Domain Transition Function**\n",
    "이 함수는 Focus가 있는 다음 도메인이나 작업을 지정 합니다. 현재 결정론적으로 정의 되며, 예제는 SVM 구현되어 있습니다.\n",
    "two-deep fully connected neural network 이며 각각의 hidden layer는 80개의 node로 이루어져 있으며 active function은 tanh며 마지막  output layer는 SVN 통하며 Hinge Loss를 사용 합니다. \n",
    "\n",
    "**SVM**\n",
    "![SVM](./simpleds/svm.png)\n",
    "\n",
    "**HINGE LOSS**\n",
    "![HINGE LOSS](./simpleds/hingeloss.png)\n",
    "\n",
    "input layer는 도메인 독립적인 단어를 허용하며 hit or miss 방식으로 모든 도메인간에 공유 되는 고유한 글로벌 어휘를 나타내는 벡터를 통하고, output 계층에는 System Domain을 나타내는 3개의 Class가 존재 한다. (meta3, 레스트토랑, 호텔)\n",
    "15,000건의 데이터 대화가 생성되어 60-40건의 교육 테스트 스플릿으로 나누어 졌으며 180 epochs , 이 분류기의 초기 결과는 사용자 시뮬레이트 된 경우 87.5 % 분류 정확도를 가지고 있다고 함.\n",
    "\n",
    "**Reward Function**\n",
    "R(s, a, s′) = GR+DR−DL\n",
    "\n",
    "GR : 전체 보상 (글로벌 보상)\n",
    "DR : State에서 Action을 관찰하여 데이터와 비슷하다면 보상\n",
    "DL : 시간의 간격\n",
    "**빠르게 GOAL까지  대화를 완료 시키면 GR은 높아 질거 같다.**\n",
    "DR 점수는 Naive Bayes 분류기에서 주어진 상태에 대한 추론 (P r (a | s)) 통해서 지급된다.\n",
    "\n",
    "**reward**\n",
    "![REWARD](./simpleds/reward.png)\n",
    "\n",
    "**Model Architectures**\n",
    "We use fully-connected multilayer neural nets, trained with stochastic gradient\n",
    "descent, where nodes in the input layers depend on the vocabulary of each agent. The use\n",
    "of convolutional neural nets is work in progress. They include 2 hidden layers with 80 nodes with\n",
    "Rectified Linear Units to normalise their weights [18]. Dropout[7] and adaptive learning rates are\n",
    "also part of our work in progress. Other hyperparameters include experience replay size=10000,\n",
    "burning steps=1000, discount factor=0.7, minimum epsilon=0.001, batch size=32, and learning\n",
    "steps=30000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Experimental Results**\n",
    "\n",
    "전자 (DQN)가 단일 정책을 사용하는 반면 학습 (기준), 후자 (NDQN)는 여러 정책 (제안)을 사용합니다. 둘 다 멀티 도메인 대화\n",
    "시스템은 교육을 위해 동일한 데이터, 리소스 및 하이퍼 패러미터를 사용하지만 유일한 차이점은 두 시스템 사이에는 학습 방법 (DQN 또는 NDQN) 또는 \n",
    "상태 표현 (예 :압축).\n",
    "시스템 성능을 측정하기 위해 다음과 같은 네 가지 측정 기준(보상, 학습 시간, 평균. 태스크 성공 및 평균 대화 길이)을 사용합니다. \n",
    "150,000 개의 학습 단계 (약 8700 개의 대화)를 통해 제안 된 시스템 보고서 결과. \n",
    "우리의 결과 보고서 단일 정책을 사용하여 다중 도메인 시스템을 교육하는 것이 다중 정책을 사용하는 것보다 **2 배 어렵다는 점**\n",
    "\n",
    "첫째, 베이스 라인 정책(DQN)이 시간이 지남에 따라 향상되지 않는다는 사실에 의해 입증됩니다.\n",
    "둘째, 제안 된 시스템은 기준선보다 4.6 배 더 빠르게 배웠고 가속화되었습니다.\n",
    "압축 입력을 사용하여 4.7 배 더 빠름 이 결과는 더 나은 확장 성을 나타냅니다.\n",
    "![propose](./simpleds/propose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Demonstration Dialogue**\n",
    "\n",
    "![Demo Exam](./simpleds/demo_exam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAPER Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning\n",
    "-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 가지 방법을 사용하여 로봇이 지각하고 행동하고 의사 소통하도록 훈련\n",
    "특히 로봇이 효율성을 배울 것으로 예상되는 경우이 여러 방향을 가르쳐야 해서 상호작용하는 셋을 만든어 인간형 로봇에게 게임을하는 방법을 가르치게 되었으며 98% 이기거나 그리는 모습을 보이고 있다.\n",
    "\n",
    "'tic-tac-toe' 휴머노이드 로봇에 적용 했으며 \n",
    "\n",
    "기존에는 **multimodal deep learning**을 통해 원시적인 픽셀을 배우는 반면\n",
    "![multimodal deep learning](./simpleds/multimodaldeeplearning.png)\n",
    "\n",
    "우리의 학습방법은 **semi-decoupled** 사용했다.\n",
    "![semi-decoupled](./simpleds/semi-decoupled.png)\n",
    "\n",
    "위를 통해서 1) 지각을 배우고 2) 상호작용하는 법을 배우고 있다.\n",
    "\n",
    "원시적인 픽셀 접근 법은 새로운 그리드 크기에 대한 동작을 학습해야 하며, 다시 학습을 하여야 한다. \n",
    "근데 semi-decoupled를 사용하면 원시적으로 안 한다는거 같은데 그런 내용은 현재 까지 없음 \n",
    "\n",
    "아래는 로봇을 학습 시킨 동영상 인것으로 보인다.\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/25jdV8FN4ic/0.jpg)](https://www.youtube.com/watch?v=25jdV8FN4ic   \"demo video\")\n",
    "\n",
    "**2 Learning Approach for Physical Human-Humanoid Interaction 여기서 부터는 다음에 보자 먼가 이상하다. ^^**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAPER SimpleDS: A Simple Deep Reinforcement Learning Dialogue System\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "거의 20년 전부터 대화 관련 시스템을 강화학습 (RL) 패러 다임으로 개선 하고자 하였으며 시간의 지남에 따라서 성능이 향상 되고 있다.\n",
    "최근 기계 학습 더욱 발전하고 있으며 지능형 에이전트 생성에 대한 인간의 개입을 줄이는 방법. 특히,DRL (Deep Reinforcement Learning) 분야는 기능 학습 및 정책 학습을 동시에 수행하여 피쳐 엔지니어링에서의 노력을 줄이고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The SimpleDS Dialogue System Action**\n",
    "\n",
    "![Dialogus System](./simpleds/sdAction.png)\n",
    "\n",
    "**The SimpleDS Dialogue System State**\n",
    "\n",
    "![Dialogus System](./simpleds/bagofword.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The SimpleDS Dialogue System**\n",
    "\n",
    "![Dialogus System](./simpleds/The SimpleDS Dialogue System.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 공간에서 어휘에 따라 최대 100 개의 단어 기반\n",
    "액션 공간은 레스토랑 도메인 2에서 35개의 대화 행동을 포함하고 있으며 \n",
    "각각의 Action들은 index되어 있다.\n",
    "인사말, 정보 검색, 요청, 사과, 확인, 암묵적 동의, 확증, 정보 제공 전체 동작 세트는 제한된 동작에서 학습을 지원합니다.\n",
    "Q-Running 학습은 유효한 일련의 종작에 대해서만 업데이트 됩니다.\n",
    "\n",
    "#**예제 대화에서 훈련된 Naive Bayes Classifier를 통해 무었을 하는 것인가 ??**\n",
    "______________________________________________________________\n",
    "용 프로그램에 독립적 인 경험적 방법으로 구동되므로 도메인 간 사용이 용이하다.\n",
    "입력 레이어에서 최대 100 개의 노드 (어휘에 따라 다름),\n",
    "첫 번째 숨겨진 레이어, 두 번째 숨겨진 레이어의 노드 40 개, 노드 35 개 (액션\n",
    "세트)를 출력 레이어에 추가합니다. \n",
    "마지막으로 learning parameters는 다음과 같습니다. \n",
    "experience replay size = 10000\n",
    "discount factor = 0.7\n",
    "minimum epsilon = 0.01\n",
    "batch size = 32\n",
    "learning steps = 20000\n",
    "여러 상태 표현을 비교하는 포괄적 인 분석, action sets, reward functions 및 learning parameters는 향후 작업으로 남겨 둡니다.\n",
    "\n",
    "\n",
    "**Simulated 3000을 사용하는 SimpleDS 에이전트의 학습 곡선을 보여줍니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dialogus System](./simpleds/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategic Dialogue Management via Deep Reinforcement Learning\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![borad game](./simpleds/Strategic Dialogue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE 정리\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**interaction/SimpleActions.java ** : resources/english/SysResponses.txt 안에 있는 SysResponses 의 데이타를 :로 분해 해서 map에 넣고 분해된 데이터에서 앞 부분을 Action으로 만든다 . (Salutation/Request 등...)\n",
    "\n",
    "**SimpleInteractionPolicy.java** :  History를 저장 하고 있고 마지막 정보도 저장 하고 있다. 총 Reward도 \n",
    "\n",
    "**SimpleUserSimulator.java** :  UsrResponses.txt 를 비슷한 방식으로 분해 하고 해당 위치에서도 action을 취해 주는 구조를 가지고 있다.\n",
    "Goal까지 가기 위한 처리를 하고 있는거 같다.\n",
    "\n",
    "**learning/SimpleAgent.java** : ConvNetJS socket 통신을 위한 전달자 역활을 하고 있다.\n",
    "\n",
    "**SimpleClassifier.java** : 이 클래스는 데모 대화에서 액션 예측을위한 Naive Bayes 분류자를 구현합니다.예상 행동의 확률은 \"데이터와 유사한 행동\"을 나타내는 점수로 볼 수 있습니다.\n",
    "Naive Bayes  : 나이브 베이즈는 분류기를 만들 수 있는 간단한 기술로써 단일 알고리즘을 통한 훈련이 아닌 \n",
    "일반적인 원칙에 근거한 여러 알고리즘들을 이용하여 훈련된다. 모든 나이브 베이즈 분류기는 공통적으로 모든 특성 \n",
    "값은 서로 독립임을 가정한다. 예를 들어, 특정 과일을 사과로 분류 가능하게 하는 특성들 \n",
    "(둥글다, 빨갛다, 지름 10cm)은 나이브 베이즈 분류기에서 특성들 사이에서 발생할 수 있는 연관성이 없음을 \n",
    "가정하고 각각의 특성들이 특정 과일이 사과일 확률에 독립적으로 기여 하는 것으로 간주한다. \n",
    "weka : Data Mining Software in Java(데이터 마이닝(data mining)은 대규모로 저장된 데이터 안에서 체계적이고 \n",
    "자동적으로 통계적 규칙이나 패턴을 찾아 내는 것)\n",
    "\n",
    "\n",
    "**SimpleEnvironment.java** : 이 클래스는 SimpleDS 에이전트를 교육하기위한 환경을 구현합니다.\n",
    "  * 환경의 상태는 주어진 어휘의 단어를 기반으로합니다.\n",
    "  * 환경의 동작은 예제 대화 및 휴리스틱에 기반합니다.\n",
    "  * 런타임에 예제 대화 상자에서 교육 인스턴스를 추출합니다.\n",
    "  \n",
    "**main/SimpleDS.java**  \n",
    "이것은 SimpleDS 학습 에이전트를 구현하기위한 주요 클래스입니다.\n",
    "  * 현재 JavaScript 클라이언트 ( 'SimpleAgent')와의 통신을 설정합니다.\n",
    "  * 또한 socketServer를 통해 Android 앱과 음성 기반 통신을 설정합니다.  \n",
    "String rewards = environment.interactionPolicy.getRewards(stateWithoutNoise, actions, end, steps);\t\tsimpleAgent.sendMessage(\"state=\"+stateWithNoise+\"|actions=\"+actions+\"|rewards=\"+rewards+\"|dialogues=\"+dialogues);  \n",
    "\n",
    "\n",
    "**networking**\n",
    "SimpleServer.java 클라이언트 - 서버 통신을위한 WebServer 오브젝트가 작성\n",
    "SimpleSocketHandler.java 클라이언트 - 서버 통신을위한 WebSocket 핸들러를 구현\n",
    "SimpleSocketServer.java  클라이언트 - 서버 통신을위한 SocketServer 객체 생성( 클라이언트 - 서버 통신을위한 SocketServer 객체)\n",
    "SimpleWebServer.java 클라이언트 - 서버 통신을위한 WebServer 오브젝트 생성\n",
    "\n",
    "**util**\n",
    "ConfigParser.java\n",
    "SimpleDS 에이전트의 구성 매개 변수를 구문 분석\n",
    "IOUtil.java 입출력 처리\n",
    "KeyValuePair.java 객체를 키 - 값 쌍으로 인스턴스화\n",
    "Logger.java 로그\n",
    "StringUtil.java 문자열 기반 처리\n",
    "Vocabulary.java 시스템 + 사용자 템플릿 응답에서 어휘집을 추출\n",
    "\n",
    "**config.txt** 환경 파일\n",
    "\n",
    "**web/convnet** \n",
    "convnet.js convolution 관련 네트워크가 다 들어 있음\n",
    "deepqlearn.js den 관련 네트워크가 이쪽에 들어 잇음\n",
    "util 관련 내용 적용 되어 있음\n",
    "\n",
    "**web/main**\n",
    "runclient.js\n",
    "createBrain ==> 네트워크 구성\n",
    "다음 기능을 사용하여 클라이언트 DRL 에이전트 구현 :\n",
    "  * + Java 기반 웹 서버와 통신합니다.\n",
    "  * + ConvNetJS를 통해 비선형 정책을 학습합니다.\n",
    "  * + 테스트 정책을 저장하고로드합니다.\n",
    "  * + 전체 및 제한된 작업 공간을 지원합니다.\n",
    "  * + 학습 곡선을 플로팅하기위한 정보를 생성합니다. \n",
    "\n",
    "**web/node_modules/ws/**\n",
    "node.js를위한 가장 빠른 WebSocket 라이브러리\n",
    "\n",
    "**web/util**\n",
    "javascript 용 유틸 폴더\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing Atari with Deep Reinforcement Learning https://arxiv.org/pdf/1312.5602v1.pdf\n",
    "\n",
    "ConvNetJS http://cs.stanford.edu/people/karpathy/convnetjs/\n",
    "\n",
    "Multi Domain Dialog System https://arxiv.org/pdf/1611.08675v1.pdf\n",
    "\n",
    "Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning\n",
    "https://arxiv.org/pdf/1611.08666.pdf\n",
    "\n",
    "Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation http://cvlab.postech.ac.kr/research/decouplednet/\n",
    "\n",
    "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System https://arxiv.org/pdf/1601.04574v1.pdf\n",
    "\n",
    "Strategic Dialogue Management via Deep Reinforcement Learning https://arxiv.org/pdf/1601.04574v1.pdf\n",
    "\n",
    "SimpleDS Source & git https://github.com/cuayahuitl/SimpleDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
