{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상황에 SyntaxNet : 이해 구글의 새로운 TensorFlow NLP 모델  \n",
    "--------------------------------------------------\n",
    "2016 년 5 월 13 일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Google은 컴퓨터 시스템이 지능적인 방식으로 처리하기 위해 인간의 언어를 읽고 이해하는 방법에 대해 많은 시간을 보냅니다. NLU (Natural Language Understanding) 시스템을위한 기초를 제공하는 TensorFlow에 구현 된 오픈 소스 신경망 프레임 워크 인 SyntaxNet을 출시함으로써 우리 연구의 성과를 더 넓은 커뮤니티와 공유하게되어 매우 기쁩니다. 우리의 릴리스에는 자신의 데이터로 새로운 SyntaxNet 모델을 교육하는 데 필요한 모든 코드뿐만 아니라 Parsey McParseface도 포함되어 있습니다. Parsey McParseface는 영어 텍스트를 분석하는 데 사용할 수있는 교육 기관입니다.\n",
    "Parsey McParseface는 언어의 언어 구조를 분석하는 방법을 배우는 강력한 기계 학습 알고리즘을 기반으로하며 주어진 문장에서 각 단어의 기능적 역할을 설명 할 수 있습니다. Parsey McParseface는 전 세계에서 가장 정확한 모델이므로 NLU의 정보, 번역 및 기타 핵심 응용 프로그램을 자동 추출하는 데 관심이있는 개발자와 연구자에게 유용 할 수 있기를 바랍니다.\n",
    "\n",
    "자동차를 제어\n",
    "전자 메일의 톤을 조정\n",
    "고객 의견을 분석\n",
    "중요한 비즈니스 위험에 대한 글로벌 뉴스를 모니터링\n",
    "(구문분석 : 문장을 그것을 이루고 있는 구성 성분으로 분해하고 그들 사이의 위계 관계를 분석하여 문장의 구조를 결정)\n",
    "(Stanford CoreNLP : 구문 분석 해주는 Tool)\n",
    "(parsing model : 구문 분석 모델(예 : spaCy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What SyntaxNet does\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구문 구문 분석기는 문장의 문법 구조를 설명하여 다른 응용 프로그램이 그 이유에 대해 설명하도록 도와줍니다. 자연어는 우리의 세계 지식이 즉시 걸러 낼 수있는 많은 예기치 않은 모호성을 도입합니다. 가장 좋아하는 예 :\n",
    "\n",
    "They ate the pizza with anchovies\n",
    "![Anchovies](./images/anchovies.jpg)\n",
    "A correct parse would link “with” to “pizza”, while an incorrect parse would link “with” to “eat”\n",
    "\n",
    "변이 형 데모를 사용하여 기술을 시각적으로 살펴 보거나 구문 분석 트리를 사용한 컴퓨팅에 대한 규칙 기반 접근 방식의 간결한 예제를 참조하십시오. 단어 - 단어 관계 트리를 사용하여 간단한 문구를 인식 할 수 있으므로 word2vec와 같은 \"bag-of-words\"기술을 쉽게 확장 할 수 있습니다. 예를 들어 2015 년 Reddit에 게시 된 모든 의견을 분석하고 구문, 항목 및 단어에 대해 word2vec를 사용했습니다. 이것은 공백으로 구분 된 단어로 엄격하게 제한된 모델보다 더 유용한 멋진 개념 맵을 생성합니다.\n",
    "\n",
    "(bag-of-words : 원래 Bag of Words 기법은 문서(document)를 자동으로 분류하기 위한 방법중 하나로서, 글에 포함된 단어(word)들의 분포를 보고 이 문서가 어떤 종류의 문서인지를 판단하는 기법을 지칭한다. 예를 들어, 어떤 문서에서 '환율', '주가', '금리' 등의 단어가 많이 나온다면 이 문서는 경제학에 관련된 문서로 분류하고 '역광', '노출', '구도' 등의 단어가 많다면 사진학에 대한 문서로 분류하는 방식이다.\n",
    "bag-of-words 모델은 각 단어의 (발생 빈도)가 분류자를 훈련하기위한 피쳐로 사용되는 문서 분류 방법에서 일반적으로 사용됩니다.)\n",
    "\n",
    "SyntaxNet은 많은 NLU 시스템에서 핵심적인 첫 번째 구성 요소 인 구문 구문 분석기로서 학계에서 알려진 것의 프레임 워크입니다. 문장을 입력으로 제공하면 단어의 구문 기능을 설명하는 품사 (part-of-speech, POS) 태그로 각 단어에 태그를 지정하고 종속 구문 분석 트리에 표시된 문장 내의 단어 사이의 구문 관계를 결정합니다. 이러한 통사 관계는 문제의 문장의 기본 의미와 직접적으로 관련이있다. 아주 간단한 예를 들기 위해 Alice saw Bob에 대한 다음 종속성 트리를 고려하십시오.\n",
    "![Image](https://3.bp.blogspot.com/-M-7PIST2hq8/VzTFhESMeuI/AAAAAAAAA_s/k4wOQe0UlnwmoVnZtuU6CNHw6xLQRN7egCLcB/s1600/asawb.png)\n",
    "이 구조는 Alice와 Bob이 명사이고 saw가 동사임을 인코딩합니다. 주 동사 본은 문장의 근원이며 앨리스는 톱의 주체 (nsubj)이며 Bob은 직접 객체 (dobj)입니다. 예상대로 Parsey McParseface는이 문장을 올바르게 분석하지만 다음과 같은보다 복잡한 예도 이해합니다.\n",
    "![Image](https://4.bp.blogspot.com/-1Ntx47T1WvU/VzTF2HgbqrI/AAAAAAAAA_w/UWofRQPhqU0ITD5HPQmEVCrwsEroCN8PQCLcB/s1600/long.png)\n",
    "\n",
    "이 구조체는 Alice와 Bob이 각각 saw의 주체이자 객체라는 사실을 인코딩합니다. 또한 Alice는 동사 읽기와 관련된 절로 수정되고, saw는 임시 수정 자에 의해 수정됩니다. 의존성 구조로 인코딩 된 문법적 관계는 다양한 질문에 대한 답변을 쉽게 복구 할 수있게 해줍니다. 예를 들어 앨리스는 누구를 보았습니까?, 밥을 본 사람, 앨리스는 무엇을 읽고 있었습니까? 언제 앨리스는 밥을 보았습니까?\n",
    "\n",
    "(Parsey McParseface : 우리가 AI (인공 지능)에 대해 생각하는 것만 큼, 자연 언어를 인식하고 파싱하는 것은 여전히 좋지 않습니다. 그것이 Google이 새로운 Parsey McParseface라는 영어 구문 분석 모델을 공개하는 이유입니다.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Parsing So Hard For Computers to Get Right?\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**파싱을 어렵게 만드는 주된 문제 중 하나는 인간 언어가 현저한 모호성 수준을 보이는 것입니다.** 중간 길이의 문장 (길이가 20 또는 30 단어)이 수십, 수천 또는 수만 가지의 구문 구조를 갖는 것은 드문 일이 아닙니다. 자연 언어 파서는 어떻게 든 이러한 대안을 모두 검색해야하며 상황에 따라 가장 합리적인 구조를 찾아야합니다. 아주 간단한 예로서 Alice가 차 안에서 거리를 운전 한 문장은 적어도 두 가지 종속성 구문 분석이 가능합니다.\n",
    "![Image](https://2.bp.blogspot.com/-cXYL6RGkV_g/VzTDzbh6yEI/AAAAAAAAA_Q/1c-76sGQ124oE9njB2E6QzU6KcxDCn0KgCLcB/s1600/drovedown.png)\n",
    "첫 번째는 Alice가 차 안에서 운전하는 (정확한) 해석에 해당합니다. 두 번째는 길거리가 그녀의 차에있는 (불합리하지만 가능한) 해석에 해당합니다. 전치사는 운전이나 거리를 수정할 수 있기 때문에 모호성이 발생합니다. 이 예는 전치사구 애착 불명의 인스턴스입니다.\n",
    "\n",
    "인간은 문제가 눈에 띄지 않을 정도로 거의 모호함을 다루는 놀라운 일을합니다. 문제는 컴퓨터가 똑같이하는 것입니다. 긴 문장에서 이들과 같은 다수의 모호성은 문장의 가능한 구조의 수에있어서 조합 적 폭발을 일으키기 위해 공모한다. 대다수의 구조체는 대개 믿기지 않지만 파서에 의해 버려 져야합니다.\n",
    "\n",
    "**SyntaxNet은 모호성 문제에 신경 네트워크를 적용합니다.** **입력 된 문장은 문장의 각 단어가 고려 될 때 점차적으로 추가되는 단어 사이의 종속성을 가지고 왼쪽에서 오른쪽으로 처리됩니다.** 처리중인 각 지점에서 많은 결정이 가능할 수 있습니다 (모호성으로 인해). **신경 네트워크는 그럴듯 ​​함을 기반으로 경쟁 결정에 점수를 부여**합니다. 이러한 이유 때문에 모델에서 **빔 검색을 사용하는 것이 매우 중요**합니다. **단순히 각 지점에서 최우수 결정을 내리는 대신 여러 단계의 가설이 각 단계에서 유지되며 가설은 고려 대상이되는 다른 상위 가설을 몇 가지 고려할 때 폐기**됩니다. 단순한 구문 분석을 생성하는 **왼쪽에서 오른쪽 순서 결정의 예**가 Google 티켓을 예약 한 문장 아래에 표시됩니다.\n",
    "\n",
    "(beam search : ** 컴퓨터 과학에서 빔 검색은 제한된 세트에서 가장 유망한 노드를 확장하여 그래프를 탐색하는 경험적 검색 알고리즘입니다. 빔 검색은 메모리 요구 사항을 줄이는 최상의 우선 검색의 최적화입니다. 최적 우선 검색은 부분 솔루션이 완전한 솔루션 (목표 상태)에 얼마나 가까운지를 예측하려고하는 일부 휴리스틱에 따라 모든 부분 솔루션 (주)을 주문하는 그래프 검색입니다. 그러나 빔 탐색에서, 미리 결정된 수의 최상의 부분 솔루션 만이 후보로 유지됩니다. [1] **)\n",
    "\n",
    "SyntaxNet은 점진적으로 구문 분석을 구성하는 변환 기반 종속성 파서 Nivre (2007)입니다. 태그 지정자와 마찬가지로 왼쪽에서 오른쪽으로 단어를 처리합니다. 모든 단어는 버퍼라고하는 미처리 된 입력으로 시작합니다. 단어가 마주 치면 스택에 놓입니다. 각 단계에서 파서는 다음 세 가지 중 하나를 수행 할 수 있습니다.\n",
    "\n",
    "SHIFT: Push another word onto the top of the stack, i.e. shifting one token from the buffer to the stack.\n",
    "\n",
    "LEFT_ARC: Pop the top two words from the stack. Attach the second to the first, creating an arc pointing to the left. Push the first word back on the stack.\n",
    "\n",
    "RIGHT_ARC: Pop the top two words from the stack. Attach the second to the first, creating an arc point to the right. Push the second word back on the stack.\n",
    "\n",
    "\n",
    "![image](https://2.bp.blogspot.com/-fqtmVS97tOs/VzTEAI9BQ8I/AAAAAAAAA_U/xPj0Av64sGseS0rF4Z1BbhmS77J-HuEvwCLcB/s1600/image04.gif)\n",
    "\n",
    "![image](./images/ff_nn_schematic.png)\n",
    "\n",
    "위의 구성에서 각 블록은 고유 한 포함 행렬을 가지며 위의 구성에있는 블록은 세미콜론으로 구분됩니다. 각 블록의 크기는 brain_pos_embedding_dims 매개 변수에서 제어됩니다. 중요 사항 : 많은 간단한 NLP 모델과 달리 이것은 단어 모델의 가방이 아닙니다. 특정 기능이 삽입 행렬을 공유하기는하지만 위의 기능이 연결되므로 input.word의 해석은 input (1) .word와 상당히 다를 것임을 기억하십시오. 또한 피쳐를 추가하면 모델의 concat 레이어의 크기는 물론 첫 번째 숨겨진 레이어의 매개 변수 수가 증가한다는 것을 의미합니다.\n",
    "\n",
    "또한, 우리의 논문에서 설명한 바와 같이, 예측 정확도를 높이기 위해 학습과 검색을 긴밀하게 통합하는 것이 중요합니다. Parsey McParseface 및 기타 SyntaxNet 모델은 Google에서 TensorFlow 프레임 워크로 교육 한 가장 복잡한 네트워크 중 일부입니다. Google에서 지원하는 Universal Dependencies 프로젝트의 일부 데이터가 주어지면 자신의 컴퓨터에서 구문 분석 모델을 학습 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Parser Step 1: Local Pretraining\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 논문에서 설명한 것처럼, 모델을 훈련하는 **첫 번째 단계는 지역 의사 결정을 사용하여 사전 훈련**하는 것입니다. 이 단계에서 우리는 의존성을 사용하여 파서를 안내하고 **softmax 레이어**를 교육하여 이러한 **의존성에 따라 올바른 작업을 예측**합니다. 파서의 결정은이 설정에서 모두 독립적이므로 매우 효율적으로 수행 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Parser Step 2: Global Training\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 이 논문에서 기술 한 것처럼, 우리가 방금 훈련 한 국부적으로 정규화 된 모델에는 몇 가지 문제가있다. **가장 중요한 것은 라벨 편향 문제**입니다. 모델은 **좋은 구문 분석이 무엇인지 알지 못하며**, 결정의 할때 취할 행동 만합니다. 이것은 점수가 각 결정에 대해 **softmax를 사용하여 로컬로 정규화**되기 때문입니다.\n",
    "\n",
    "이 논문에서는 **글로벌 정규화 모델을 사용하여 훨씬 더 나은 결과를 얻을 수있는 방법**을 보여줍니다.이 모델에서 **softmax 점수는 로그 공간에서 합산되며 점수는 최종 결정에 도달 할 때까지 정규화되지 않습니다**. \n",
    "\n",
    "여러 가설을 유지하는 것이 중요한 정원 경로 문장에 대해이 훈련이 어떻게 작동하는지 간략하게 설명합니다. \n",
    "**초기에 구문 분석을 할 때 한 번 실수하면 구문 분석이 완전히 잘못됩니다. 훈련 후에 모델은 두 번째 (올바른) 구문 분석을 선호합니다.**\n",
    "![image](./images/beam_search_training.png)\n",
    "\n",
    "일단 사전 훈련 된 로컬 정규화 모델을 얻으면 이제 다음 명령으로 전역 적으로 정규화 된 구문 분석 모델을 학습 할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "참고문헌\n",
    "------\n",
    "https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html\n",
    "https://explosion.ai/about\n",
    "https://explosion.ai/blog/syntaxnet-in-context\n",
    "https://www.youtube.com/watch?v=AKwfVAKaigI&feature=youtu.be\n",
    "https://github.com/livingbio/syntaxnet_wrapper\n",
    "https://github.com/tensorflow/models/tree/master/syntaxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
